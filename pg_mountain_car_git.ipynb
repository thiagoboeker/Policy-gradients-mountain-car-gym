{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Policy Gradients With OpemAIGym"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#imports\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import gym"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2018-04-02 17:57:48,026] Making new env: MountainCar-v0\n"
     ]
    }
   ],
   "source": [
    "#Create the enviroment\n",
    "env = gym.make('MountainCar-v0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Hyperparameters\n",
    "n_inputs = 2 #obs[0] - position  obs[1] - action\n",
    "n_outputs = 3 # Number of possible actions \n",
    "learning_rate = 0.01\n",
    "n_hidden = 10\n",
    "initializer = tf.random_normal_initializer(mean = 0.0, stddev = 0.3) #Weights initializer\n",
    "b_initializer = tf.constant_initializer(0.1) #Bias initializer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.reset_default_graph() #Reset the graph for new training\n",
    "X = tf.placeholder(tf.float32, shape=(None,n_inputs)) #obs placeholder\n",
    "y = tf.placeholder(tf.int64, shape = (None)) #choosed action in training placeholder\n",
    "hidden1 = tf.layers.dense(X, n_hidden, kernel_initializer = initializer, bias_initializer = b_initializer, activation = tf.nn.tanh)\n",
    "logits = tf.layers.dense(hidden1, n_outputs, kernel_initializer = initializer, bias_initializer = b_initializer)\n",
    "outputs = tf.nn.softmax(logits) #Probabilitys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "action = tf.multinomial(tf.log(outputs), num_samples = 3) \n",
    "#This actually display 3 integers\n",
    "#Which one will be the index of the probability target, see documentation\n",
    "\n",
    "loss = tf.nn.sparse_softmax_cross_entropy_with_logits(logits = outputs, labels = y) #Loss function\n",
    "\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate = learning_rate) # The optimizer\n",
    "grad_and_vars = optimizer.compute_gradients(loss) # Get the gradients and vars of the loss function\n",
    "gradients = [grad for grad,var in grad_and_vars] #Get the gradients\n",
    "gradients_placeholders = [] # To store the gradients\n",
    "grad_and_vars_feed = [] # To store the placeholders with the vars\n",
    "for grad, var in grad_and_vars:\n",
    "    \n",
    "    grad_placeholder = tf.placeholder(tf.float32, shape = grad.get_shape()) #The respective grad placeholder\n",
    "    gradients_placeholders.append(grad_placeholder) # Store the placeholder\n",
    "    grad_and_vars_feed.append((grad_placeholder, var)) #Store the placeholder with his respective var\n",
    "\n",
    "training_op = optimizer.apply_gradients(grad_and_vars_feed) # OP to apply the placeholder value with each var"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Function that apply the algorithm for future discount reward\n",
    "def discount_rewards(rewards, discount_rate):\n",
    "    discounted_rewards = np.empty(len(rewards))\n",
    "    cumulative_rewards = 0\n",
    "    for step in reversed(range(len(rewards))):\n",
    "        cumulative_rewards = cumulative_rewards * discount_rate + rewards[step]\n",
    "        discounted_rewards[step] = cumulative_rewards\n",
    "    \n",
    "    return discounted_rewards"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function apply the discount_rewards for every game in the n_game_per_iter, then normalize it\n",
    "def discount_and_normalized_rewards(all_rewards, discount_rate):\n",
    "    all_discount_rewards = [discount_rewards(rewards, discount_rate) for rewards in all_rewards]\n",
    "    flatten = np.concatenate(all_discount_rewards)\n",
    "    mean = flatten.mean()\n",
    "    std = flatten.std()\n",
    "    return [(discounted_rewards - mean)/std for discounted_rewards in all_discount_rewards]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "iterations = 1001\n",
    "n_games_per_iter = 10 # Each iteration will play the game 10 times and store the rewards for every game\n",
    "discount_rate = 0.95\n",
    "init = tf.global_variables_initializer()\n",
    "saver = tf.train.Saver()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New maxHeight: -0.4076945609163165\n",
      "New maxHeight: -0.40585271170142306\n",
      "New maxHeight: -0.39680886411668714\n",
      "New maxHeight: -0.3876931760403401\n",
      "New maxHeight: -0.3795687737623642\n",
      "New maxHeight: -0.3714912955693839\n",
      "New maxHeight: -0.3655154419403419\n",
      "New maxHeight: -0.3596812666861505\n",
      "New maxHeight: -0.35602752012340055\n",
      "New maxHeight: -0.3535782868716556\n",
      "New maxHeight: -0.3513496308454897\n",
      "New maxHeight: -0.35035611200831396\n",
      "New maxHeight: -0.347871126876211\n",
      "New maxHeight: -0.34489381874141595\n",
      "New maxHeight: -0.3431935101330151\n",
      "New maxHeight: -0.34278114755094397\n",
      "Rewards: -982.0\n",
      "New maxHeight: -0.3356796260641141\n",
      "New maxHeight: -0.32827119240740765\n",
      "New maxHeight: -0.32224530486504704\n",
      "New maxHeight: -0.3166393898536296\n",
      "New maxHeight: -0.31148784896637266\n",
      "New maxHeight: -0.30682193300556243\n",
      "New maxHeight: -0.3046696407959586\n",
      "New maxHeight: -0.3030437880035077\n",
      "New maxHeight: -0.29938487020455123\n",
      "New maxHeight: -0.2960955485763982\n",
      "New maxHeight: -0.29438308320143086\n",
      "New maxHeight: -0.29048474756056536\n",
      "New maxHeight: -0.28128648709561765\n",
      "New maxHeight: -0.27274968648219855\n",
      "New maxHeight: -0.264921636753137\n",
      "New maxHeight: -0.25884471844582\n",
      "New maxHeight: -0.25355116693996094\n",
      "New maxHeight: -0.25006857954727263\n",
      "New maxHeight: -0.24941486368947557\n",
      "New maxHeight: -0.23040664014284906\n",
      "New maxHeight: -0.21056842879196905\n",
      "New maxHeight: -0.19174777162458292\n",
      "New maxHeight: -0.17402476434341566\n",
      "New maxHeight: -0.15946872365832712\n",
      "New maxHeight: -0.14613200745550972\n",
      "New maxHeight: -0.1360588754938301\n",
      "New maxHeight: -0.12828035881098926\n",
      "New maxHeight: -0.12381898739356502\n",
      "New maxHeight: -0.1226871147167422\n",
      "New maxHeight: -0.11759805254734249\n",
      "New maxHeight: -0.11066365422162334\n",
      "New maxHeight: -0.10709274417430413\n",
      "New maxHeight: -0.10489391551077498\n",
      "New maxHeight: -0.1040723241745655\n",
      "New maxHeight: -0.09863739626324691\n",
      "New maxHeight: -0.09352458604357702\n",
      "New maxHeight: -0.0898140176209826\n",
      "New maxHeight: -0.08751324811935388\n",
      "New maxHeight: -0.08662681347573405\n",
      "Rewards: -891.0\n",
      "New maxHeight: -0.08555944530735357\n",
      "New maxHeight: -0.07688496603984246\n",
      "New maxHeight: -0.07164427898274624\n",
      "New maxHeight: -0.06984606872831929\n",
      "Rewards: -961.0\n",
      "Rewards: -925.0\n",
      "New maxHeight: -0.06157221476024781\n",
      "New maxHeight: -0.05170176180356713\n",
      "New maxHeight: -0.045301297025201574\n",
      "New maxHeight: -0.0423777804254548\n",
      "New maxHeight: -0.04193408741535642\n",
      "Rewards: -909.0\n",
      "Rewards: -909.0\n",
      "New maxHeight: -0.038613301556139255\n",
      "New maxHeight: -0.03648575898912466\n",
      "New maxHeight: -0.029985497275666333\n",
      "New maxHeight: -0.011938012544375962\n",
      "New maxHeight: 0.002611075322163413\n",
      "New maxHeight: 0.013660239887596907\n",
      "New maxHeight: 0.021211503433480126\n",
      "New maxHeight: 0.026267826960174233\n",
      "New maxHeight: 0.02883190895636939\n",
      "New maxHeight: 0.028905337011955478\n",
      "Rewards: -970.0\n",
      "New maxHeight: 0.03439785780824281\n",
      "New maxHeight: 0.04357911047979777\n",
      "New maxHeight: 0.050281698049204265\n",
      "New maxHeight: 0.05451267452966673\n",
      "New maxHeight: 0.05527700742482577\n",
      "New maxHeight: 0.0597017190709692\n",
      "New maxHeight: 0.06285615220025771\n",
      "New maxHeight: 0.0635549013582691\n",
      "New maxHeight: 0.06837568804263615\n",
      "New maxHeight: 0.07456908967856977\n",
      "New maxHeight: 0.07732478704206298\n",
      "New maxHeight: 0.07764744843652391\n",
      "New maxHeight: 0.07875033804360383\n",
      "New maxHeight: 0.07924499280594753\n",
      "Rewards: -957.0\n",
      "New maxHeight: 0.08062297462683157\n",
      "New maxHeight: 0.09197382981413955\n",
      "New maxHeight: 0.09991924859792267\n",
      "New maxHeight: 0.10547614725144322\n",
      "New maxHeight: 0.10765716376932181\n",
      "New maxHeight: 0.11253187240688545\n",
      "New maxHeight: 0.13800939383160188\n",
      "New maxHeight: 0.16119814596697768\n",
      "New maxHeight: 0.18217357470283058\n",
      "New maxHeight: 0.20101315861015442\n",
      "New maxHeight: 0.2177937035474567\n",
      "New maxHeight: 0.23158916586006859\n",
      "New maxHeight: 0.24246412174219162\n",
      "New maxHeight: 0.2504718005760098\n",
      "New maxHeight: 0.25565267105083206\n",
      "New maxHeight: 0.25903347907833363\n",
      "New maxHeight: 0.2596319127321247\n",
      "Rewards: -941.0\n",
      "New maxHeight: 0.27007993276575804\n",
      "New maxHeight: 0.28368924134912676\n",
      "New maxHeight: 0.2946505984082937\n",
      "New maxHeight: 0.30402670453041863\n",
      "New maxHeight: 0.31187255523900725\n",
      "New maxHeight: 0.31823510259793325\n",
      "New maxHeight: 0.3231530267216706\n",
      "New maxHeight: 0.3256565867317059\n",
      "New maxHeight: 0.3257613052624467\n",
      "New maxHeight: 0.3267575257867192\n",
      "New maxHeight: 0.32762637313623616\n",
      "New maxHeight: 0.3310720230678033\n",
      "New maxHeight: 0.34323500715908256\n",
      "New maxHeight: 0.35411031198196713\n",
      "New maxHeight: 0.3637685234276869\n",
      "New maxHeight: 0.3722734163251662\n",
      "New maxHeight: 0.3796819533871303\n",
      "New maxHeight: 0.3850443372561976\n",
      "New maxHeight: 0.3883972289951444\n",
      "New maxHeight: 0.39076368469856443\n",
      "New maxHeight: 0.39216003747001743\n",
      "New maxHeight: 0.3968504045879465\n",
      "New maxHeight: 0.40278729841047417\n",
      "New maxHeight: 0.4068378133231923\n",
      "New maxHeight: 0.4100304194111698\n",
      "New maxHeight: 0.4123876462094731\n",
      "New maxHeight: 0.41292617745537236\n",
      "Rewards: -923.0\n",
      "New maxHeight: 0.4183458565109765\n",
      "New maxHeight: 0.42594601718950503\n",
      "New maxHeight: 0.43282421357091155\n",
      "New maxHeight: 0.4390299844133641\n",
      "New maxHeight: 0.44460827174808737\n",
      "New maxHeight: 0.4485996594452761\n",
      "New maxHeight: 0.4510332876989036\n",
      "New maxHeight: 0.4529269633753457\n",
      "New maxHeight: 0.4532945624655376\n",
      "Rewards: -945.0\n",
      "New maxHeight: 0.4580048934859313\n",
      "New maxHeight: 0.4634873178924624\n",
      "New maxHeight: 0.46752134596325806\n",
      "New maxHeight: 0.47113677439276297\n",
      "New maxHeight: 0.47436036018052724\n",
      "New maxHeight: 0.4762159993533582\n",
      "New maxHeight: 0.4767174632750178\n",
      "New maxHeight: 0.4829380833708277\n",
      "New maxHeight: 0.48797916717569145\n",
      "New maxHeight: 0.49175361204404977\n",
      "New maxHeight: 0.495289581334691\n",
      "New maxHeight: 0.4976134868082812\n",
      "New maxHeight: 0.4987426999505291\n",
      "New maxHeight: 0.5052765691491946\n",
      "New maxHeight: 0.5081915030056045\n",
      "New maxHeight: 0.5103348710320418\n",
      "New maxHeight: 0.5249385718063834\n",
      "Rewards: -928.0\n",
      "New maxHeight: 0.5254765880197055\n",
      "New maxHeight: 0.5313453843775382\n",
      "Rewards: -889.0\n",
      "New maxHeight: 0.5386625546678055\n",
      "Rewards: -865.0\n",
      "New maxHeight: 0.5389330428453711\n",
      "Rewards: -796.0\n",
      "New maxHeight: 0.5389821843785622\n",
      "New maxHeight: 0.5392182204628733\n",
      "Rewards: -807.0\n",
      "New maxHeight: 0.5401216274904896\n",
      "New maxHeight: 0.5410629624046291\n",
      "New maxHeight: 0.5414505739143163\n",
      "Rewards: -837.0\n",
      "Rewards: -839.0\n",
      "Rewards: -817.0\n",
      "New maxHeight: 0.5424232351465018\n",
      "Rewards: -771.0\n",
      "Rewards: -844.0\n"
     ]
    }
   ],
   "source": [
    "\n",
    "with tf.Session() as sess:\n",
    "    \n",
    "    init.run()\n",
    "    iter_height = -0.45\n",
    "    for iteration in range(iterations):\n",
    "        \n",
    "        all_rewards = [] # Store all the 10 games rewards\n",
    "        all_gradients = [] # Store all the 10 games gradients values\n",
    "        rewards_sum = [] # Store the rewards for tracking improvements\n",
    "        \n",
    "        for game in range(n_games_per_iter):\n",
    "            \n",
    "            current_rewards = [] # Store the rewards for the game\n",
    "            current_gradients = [] # Store the gradients of the game\n",
    "            \n",
    "            obs = env.reset() # Reset enviroment\n",
    "            \n",
    "            while True:\n",
    "                \n",
    "                previousHeight = obs[0] # Store the previous height\n",
    "                \n",
    "                all_acts = sess.run(action, feed_dict = {X:obs.reshape(1, n_inputs)}) # run the tf.multinomial function\n",
    "                \n",
    "                choosed_action = np.random.choice(all_acts.ravel()) # choose a random action\n",
    "                \n",
    "                # Run the gradients\n",
    "                gradients_val = sess.run(gradients, feed_dict = {X:obs.reshape(1, n_inputs), y:np.array([choosed_action])})\n",
    "                \n",
    "                # Run the step with the choosed action\n",
    "                obs_, reward, done, info = env.step(choosed_action)\n",
    "                \n",
    "                if obs_[0] > previousHeight:\n",
    "                    #This is tricky, the env dont give any positive reward to the agent, its just -1 for each time step\n",
    "                    #So i give a positive reward every time the agente reachs a higher height then the previou\n",
    "                    reward += 1\n",
    "                    \n",
    "                if obs_[0] > iter_height: # Compare with max height for all iterations\n",
    "                    # I'm tracking the max height that ever ocurred\n",
    "                    iter_height = obs_[0] # Update if is the case\n",
    "                    print(\"New maxHeight: {}\".format(iter_height)) #Print\n",
    "                    reward += 2 # I'm giving a even bigger reward when it's reaches the max height for all the iteration\n",
    "                    # Hoping this helps the agente figure it out \n",
    "                    \n",
    "                current_rewards.append(reward) # Append the step reward\n",
    "                rewards_sum.append(reward) \n",
    "                current_gradients.append(gradients_val) # Append the step gradient val\n",
    "                obs = obs_ # Upadate the previous obs\n",
    "                \n",
    "                if done:\n",
    "                    all_rewards.append(current_rewards) # Append all the steps rewards for the game\n",
    "                    all_gradients.append(current_gradients) # Append all the steps grad vals for the game\n",
    "                    break\n",
    "          \n",
    "        all_rewards = discount_and_normalized_rewards(all_rewards, discount_rate) # Apply discount and normalize the rewards\n",
    "        feed_dict = {}\n",
    "        for var_index, grad_placeholder in enumerate(gradients_placeholders):\n",
    "            \n",
    "            #Calculate the mean of all the grad values for each game and each game step\n",
    "            mean_gradient = np.mean([reward * all_gradients[game_index][step][var_index] for game_index, rewards\\\n",
    "                                     in enumerate(all_rewards) for step, reward in enumerate(rewards)], axis = 0)\n",
    "            \n",
    "            feed_dict[grad_placeholder] = mean_gradient # Store the value in the respective grad placeholder\n",
    "        \n",
    "        sess.run(training_op, feed_dict = feed_dict) # Run the optimizer for the apply_gradients function\n",
    "        \n",
    "        if iteration % 50 == 0:\n",
    "            print(\"Rewards: {}\".format(sum(rewards_sum))) # Print the sum of the rewards for all games\n",
    "            saver.save(sess, './logs/pg_net7.ckpt') # Save the iteration\n",
    "            \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from ./logs/pg_net7.ckpt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2018-04-02 18:00:54,467] Restoring parameters from ./logs/pg_net7.ckpt\n"
     ]
    }
   ],
   "source": [
    "#Code to see the agente playing the game\n",
    "with tf.Session() as sess:\n",
    "    saver.restore(sess, './logs/pg_net7.ckpt')\n",
    "    \n",
    "    for game in range(30):\n",
    "        \n",
    "        obs = env.reset()\n",
    "        \n",
    "        while True:\n",
    "            \n",
    "            action = sess.run(outputs, feed_dict = {X:obs.reshape(1, n_inputs)})\n",
    "            choose_action = np.argmax(action, 1)\n",
    "            obs, reward, done, info = env.step(choose_action[0])\n",
    "            env.render()\n",
    "            if done:\n",
    "                break"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "In the training didnt showed that he surpass the limit of 0.5 but that was because of the low number of iteractions, so most of the time the agente will win the game as suposed, but sometimes he will didnt even realize what to do or just try and lose, with more training the agente certainly perform better, but my cpu power is limited now. With 1000 iterations did it perfectly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
